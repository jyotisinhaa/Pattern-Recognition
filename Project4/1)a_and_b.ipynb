{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1)a_and_b.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTjAKzinJDvr",
        "colab_type": "text"
      },
      "source": [
        "<h2><strong>Problem 1</strong></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-8a3t5PI0J-",
        "colab_type": "text"
      },
      "source": [
        "<strong>Demonstrate that a neural network to maximize the log likelihood of label is one that has softmax output nodes and minimizes the criterion function of the negative log probability of training data set:</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFGPAV8JMpWm",
        "colab_type": "text"
      },
      "source": [
        "Ans:- a) In order to prove this , first I am explaining certain terms:-\n",
        "\n",
        "Sigmoid Activation Function:-\n",
        "  We know that at the output layer of neural network the softmax activation function is placed. It‚Äôs mostly used in multi-class learning problems where a set of features can have k classes. The equation of the sigmoid function is simple and we find the normalized exponential function of all the units in the layer and is given by,\n",
        "$$\\begin{align}\n",
        " S(f_yi)=  -\\dfrac{e^f_yi}{\\sum_{j}e^f_j}\n",
        "\\end{align}$$ \n",
        "\n",
        "We can say that the output of the softmax as the probabilities that a certain set of features belongs to a certain class.We can then see that one advantage of using the softmax at the output layer is that it improves the interpretability of the neural network. \n",
        "\n",
        "Negative Log Likelihood:-\n",
        "\n",
        "Likelihood function is the product of probability distribution function which assume that each observation is independent.Loss function is very interesting if we interpret it in relation to the behavior of softmax. \n",
        "The loss function is given by,\n",
        "\n",
        "L(y)= -log(y)\n",
        "\n",
        "\n",
        "This is summed for all the correct classes.\n",
        "\n",
        "We know that higher loss increases the unhappiness. Thus, we try to minimize the los function.Similiarly, we try to maximize the negative log-likelihood to make the model happy.Now, I am trying  to show how the loss changes with respect to the output of the network.Thus, differentitating the loss function wrt to the output Sigmoid function we get,\n",
        "$$\\begin{align}\n",
        " \\frac{\\mathrm d L_i}{\\mathrm d f_k} = \\frac{\\mathrm d L_i}{\\mathrm d p_k}\\frac{\\mathrm d p_k}{\\mathrm d f_k}\n",
        "\\end{align}$$ \n",
        "\n",
        "The derivative of loss is given by,\n",
        "\n",
        "$$\\begin{align}\n",
        " \\frac{\\mathrm d L_i}{\\mathrm d p_k} = -\\dfrac{1}{p_k}\n",
        "\\end{align}$$ \n",
        "And, the derivative of second term is given by,\n",
        "$$\\begin{align}\n",
        " \\frac{\\mathrm d p_k}{\\mathrm d f_k} = p_k*(1- p_k)\n",
        "\\end{align}$$ \n",
        "on merging both the terms we get,\n",
        "$$\\begin{align}\n",
        " \\frac{\\mathrm d L_i}{\\mathrm d f_k} = \\frac{\\mathrm d L_i}{\\mathrm d p_k}\\frac{\\mathrm d p_k}{\\mathrm d f_k}=(p_k - 1)\n",
        "\\end{align}$$ \n",
        "Thus, we can easily see that maximising the likelihood has softmax output node.In another way,We know that the Maximum likelihood is a generative training criterion by\n",
        "which the model learns the likelihood of correct class for the\n",
        "observation. The model makes predictions by using Bayes\n",
        "rules to calculate posterior probabilities of target classes for\n",
        "the observation and then select the most likely class. Suppose ùíô is the feature vector and ùë¶ is the class that the\n",
        "observation belongs to x. By chosing the independently and\n",
        "identically distributed (i.i.d.) samples from the data space and\n",
        "generate a training set\n",
        "ùëÜ = {(ùíôùüè, ùë¶ùüè), ‚ãØ , (ùíôm, ùë¶ùëö)}\n",
        "For binary classification task where there are two target\n",
        "classes, a model is designed with one single output for\n",
        "which the empirical probability is equal to 1. Thus, the binary cross-entropy loss function is given by,\n",
        "$$\\begin{align}\n",
        " L(f(x),y)= -p(y_c|x)logp(y_v|x)= -[1-p(y_c|x)]log[1-p(y_c|x)]\n",
        "\\end{align}$$ \n",
        "For multiclass, this can be written as,\n",
        "$$\\begin{align}\n",
        " L(f(x),y)= -p(y_c|x)-\\sum_{k=1,k!=c}^Clog[1-p(y_k|x)]\n",
        "\\end{align}$$ \n",
        "To overcome the shortage of the\n",
        "generative training criterion, we propose to measure the ratio\n",
        "between the predicted correct-class probability and the\n",
        "competing ones in the loss function, defined as\n",
        "$$\\begin{align}\n",
        " L(f(x),y)= log\\dfrac{p(y_c|x)}{\\sum_{k=1,k!=c}^Cp(y_k|x)}\n",
        "\\end{align}$$\n",
        "The denominator on the right side is the sum of competing class probabilities representing the probability of not belonging to the correct class, p(y_c|x').\n",
        "Thus, the above equation can be reduced as,\n",
        "$$\\begin{align}\n",
        " L(f(x),y)= -logp(x|y_c)-logp(x|y_c')\n",
        "\\end{align}$$\n",
        "Thus, it indicates that the log function is negative log likelihood ratio between correct and competing class.Hence, it is proved that to maximize the log likelihood of label that has softmax output nodes and minimizes the criterion function of the negative log probability of training data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p337cCiVKE8d",
        "colab_type": "text"
      },
      "source": [
        "<h2><strong>Problem 2</strong><h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bENfJdPZI4pH",
        "colab_type": "text"
      },
      "source": [
        "<strong>Demonstrate that a neural network to maximize the a posterior likelihood of observing the training data given a Gaussian prior is one that minimizes the criterion function with L2 regularization.</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49n2UWzBJsqs",
        "colab_type": "text"
      },
      "source": [
        "Ans:- b)When given a guassian prior of weight distribution:\n",
        "\n",
        "Consider a single continuous target variable t from\n",
        "a vector x of inputs. We are assuming that the  conditional distribution p(t|x) is Gaussian, with an x-dependent mean given by the output of a neural network model y(x, w), and is given by\n",
        "$$\\begin{align}\n",
        "  P(t|x,w)=  \\mathcal{N}(t|y(x,w), \\beta^-1)\n",
        "\\end{align}$$\n",
        "where $\\beta$ is the inverse variance of the Gaussian noise.\n",
        "\n",
        "Here,  prior  distribution  is chosen over the weights w that is the Gaussian  and it is in the form,\n",
        "$$\\begin{align}\n",
        " P(w|\\alpha )=  \\mathcal{N}(w|0,\\alpha^-1I)\n",
        "\\end{align}$$        \n",
        "For an i.i.d. dataset of N observations $x_1, ...., x_n$ with the corresponding set of target values D= {$t_1,....,t_N$}, the likelihood function is given by,\n",
        "\n",
        "$$\\begin{align}\n",
        " P(D|w,\\beta )= \\sum_{n=1}^N \\mathcal{N}(t_n|y(x_n,w), \\beta^-1)\n",
        "\\end{align}$$\n",
        "So, the resulting posterior distribution of weight is given by combining the prior and the likelihood. Thus, the posterior distribution is given by,\n",
        "$$\\begin{align}\n",
        " P(w|D;\\alpha,\\beta ) = p(D|w;\\beta) p(w;\\alpha)/ p(D;\\alpha,\\beta )\n",
        "                      = \\mathcal{N}(w|0,\\alpha^-1I)\\sum_{n=1}^N\\mathcal{N}(t_n|y(x_n,w), \\beta^-1)/p(D;\\alpha,\\beta )\n",
        "\\end{align}$$\n",
        "The Gaussian approximation to the posterior distribution can be determined by using the\n",
        "Laplace approximation. In order to acheive this, we first find a (local) maximum of the\n",
        "posterior, and this is done using iterative numerical optimization.It\n",
        "is convenient to maximize the logarithm of the posterior, which can be given,\n",
        "$$\\begin{align}\n",
        " lnp(w|D)=-\\dfrac{\\alpha}{2}w^tw - \\dfrac{\\beta}{2}\\sum_{n=1}^N \\{y(x_n,w)- t_n\\}^2 + const\n",
        "\\end{align}$$\n",
        "On maximising the above expression with respect to $\\beta$ we get the maximum a-posteriori estimate for $\\beta$. From this expression it is clear  that why the Gaussian prior can be interpreted as a L2 regularisation term. Since, multivariate normal prior and multivariate normal likelihood, leads to multivariate normal posterior distribution in which the mean of the posterior  is exactly that we would obtain using ùêø2 regularized  with an appropriate regularization parameter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW3zEwBSlGoR",
        "colab_type": "text"
      },
      "source": [
        "<h3><strong> References</strong></h3>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyfQxc7zlMUr",
        "colab_type": "text"
      },
      "source": [
        "1) https://arxiv.org/pdf/1804.10690.pdf\n",
        "\n",
        "2)http://d2l.ai/chapter_linear-networks/softmax-regression.html\n",
        "\n",
        "3)https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
        "\n",
        "4)http://users.isr.ist.utl.pt/~wurmd/Livros/school/\n",
        "Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\n",
        "\n",
        "5)Slide of the professor\n"
      ]
    }
  ]
}