{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Cnnautoenoder.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8u8Q8AlY8AV",
        "colab_type": "text"
      },
      "source": [
        "<h1><strong>CSE 555 - Introduction to Pattern Recognition</strong></h1>\n",
        "\n",
        "<h2><strong>Programming Assignment 5 :- Generative Models</strong></h2>\n",
        "<h4><strong> Jyoti Sinha</strong></h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GerFREmbYqaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!git clone https://github.com/tensorflow/tensorflow.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK0Ur8gMYqaR",
        "colab_type": "text"
      },
      "source": [
        "<h3>Importing packages</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrCIQfBaYqaR",
        "colab_type": "code",
        "colab": {},
        "outputId": "8934162b-3592-4b9c-81b0-889453b37008"
      },
      "source": [
        "!pip install -U dm-sonnet==1.23\n",
        "!pip install --upgrade tensorflow-probability\n",
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: dm-sonnet==1.23 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (1.23)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from dm-sonnet==1.23) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from dm-sonnet==1.23) (1.12.0)\n",
            "Requirement already up-to-date: tensorflow-probability in /Users/jyoti/anaconda3/lib/python3.7/site-packages (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow-probability) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow-probability) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow-probability) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow-probability) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle>=1.2.2 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow-probability) (1.3.0)\n",
            "Collecting tensorflow==2.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/2c/72/6b3264aa2889b7dde7663464b99587d95cd6a5f3b9b30181f14d78a63e64/tensorflow-2.0.0-cp37-cp37m-macosx_10_11_x86_64.whl\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (2.0.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.16.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (0.33.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (2.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.24.1)\n",
            "Requirement already satisfied: h5py in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (41.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.15.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.22.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.14.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /Users/jyoti/anaconda3/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW9BVHkuYqaX",
        "colab_type": "text"
      },
      "source": [
        "<h3> Importing</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk9fzM6rYqaY",
        "colab_type": "code",
        "colab": {},
        "outputId": "0436fed6-092a-44eb-df91-d8d55081edc7"
      },
      "source": [
        "import input_data\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /Users/jyoti/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE8vct3fYqab",
        "colab_type": "text"
      },
      "source": [
        "<h3> Importing dataset</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg72Z-NcYqac",
        "colab_type": "code",
        "colab": {},
        "outputId": "477980d2-d5db-4ec8-897d-26b97345fc5e"
      },
      "source": [
        "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
        "train_X, train_Y, test_X, test_Y = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-2cf396ef0b1e>:1: read_data_sets (from input_data) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as: tensorflow_datasets.load('mnist')\n",
            "WARNING:tensorflow:From /Users/jyoti/Desktop/PRnew/input_data.py:297: _maybe_download (from input_data) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /Users/jyoti/Desktop/PRnew/input_data.py:299: _extract_images (from input_data) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /Users/jyoti/Desktop/PRnew/input_data.py:304: _extract_labels (from input_data) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /Users/jyoti/Desktop/PRnew/input_data.py:112: _dense_to_one_hot (from input_data) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /Users/jyoti/Desktop/PRnew/input_data.py:328: _DataSet.__init__ (from input_data) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/_DataSet.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTcE4xinYqai",
        "colab_type": "text"
      },
      "source": [
        "<h3>Displaying some digits</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQOliFi8Yqai",
        "colab_type": "code",
        "colab": {},
        "outputId": "0f24d056-c644-42c3-f7dd-4481a37ba7b5"
      },
      "source": [
        "img = mnist.train.images[5]\n",
        "plt.imshow(img.reshape((28, 28)), cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x10d92cf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyTxW0JUYqam",
        "colab_type": "code",
        "colab": {},
        "outputId": "a65bbffa-9636-4fdd-9969-851166f3be39"
      },
      "source": [
        "img = mnist.train.images[8]\n",
        "plt.imshow(img.reshape((28, 28)), cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1335379e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMENX3zHYqap",
        "colab_type": "text"
      },
      "source": [
        "<h3>Encoding</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H48nz8MXYqaq",
        "colab_type": "code",
        "colab": {},
        "outputId": "57045b38-4e65-496d-ab23-a2064103805a"
      },
      "source": [
        "inputs_dim = tf.placeholder(tf.float32, (None, 28, 28, 1), name='input_dim')\n",
        "output = tf.placeholder(tf.float32, (None, 28, 28, 1), name='output')\n",
        "convoluion1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(inputs_dim)\n",
        "maxpooling1 = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(convoluion1)\n",
        "convoluion2 = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(maxpooling1)\n",
        "maxpooling2 = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(convoluion2)\n",
        "convoluion3 = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(maxpooling2)\n",
        "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(convoluion3) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /Users/jyoti/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77JkKdhnYqau",
        "colab_type": "text"
      },
      "source": [
        "<h3> Decoding</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGY2cAjaYqav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convoluion4= tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "upsample1 = tf.keras.layers.UpSampling2D((2, 2))(convoluion4)\n",
        "convoluion5 = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(upsample1 )\n",
        "upsample2 = tf.keras.layers.UpSampling2D((2, 2))(convoluion5)\n",
        "convoluion6 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(upsample2)\n",
        "upsample3 = tf.keras.layers.UpSampling2D((2, 2))(convoluion6)\n",
        "convolution7 = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(upsample3 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ9N6iT5Yqay",
        "colab_type": "text"
      },
      "source": [
        "<h3>Calculation of error,opt and cost</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goI5xkgaYqaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss= tf.losses.mean_squared_error(output,convolution7 )\n",
        "cost = tf.reduce_mean(loss)\n",
        "opt = tf.train.AdamOptimizer(0.001).minimize(cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV5DxT07Yqa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnw-3_3DYqa5",
        "colab_type": "code",
        "colab": {},
        "outputId": "8177b306-abb9-4567-d948-0bfa7e72745b"
      },
      "source": [
        "batchsize = 200\n",
        "epochs=2\n",
        "sess.run(tf.global_variables_initializer())\n",
        "for a in range(epochs):\n",
        "    for b in range(mnist.train.num_examples//batchsize):\n",
        "        batch_dim = mnist.train.next_batch(batchsize)\n",
        "        image_dim = batch_dim[0].reshape((-1, 28, 28, 1))\n",
        "        costbatch, _ = sess.run([cost, opt], feed_dict={inputs_dim: image_dim ,output: image_dim })\n",
        "        print(\"Epoch: {}/{}...\".format(a+1, epochs),\"Training loss: {:.4f}\".format(costbatch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/2... Training loss: 0.2333\n",
            "Epoch: 1/2... Training loss: 0.2307\n",
            "Epoch: 1/2... Training loss: 0.2286\n",
            "Epoch: 1/2... Training loss: 0.2274\n",
            "Epoch: 1/2... Training loss: 0.2246\n",
            "Epoch: 1/2... Training loss: 0.2216\n",
            "Epoch: 1/2... Training loss: 0.2175\n",
            "Epoch: 1/2... Training loss: 0.2133\n",
            "Epoch: 1/2... Training loss: 0.2077\n",
            "Epoch: 1/2... Training loss: 0.2007\n",
            "Epoch: 1/2... Training loss: 0.1913\n",
            "Epoch: 1/2... Training loss: 0.1831\n",
            "Epoch: 1/2... Training loss: 0.1721\n",
            "Epoch: 1/2... Training loss: 0.1611\n",
            "Epoch: 1/2... Training loss: 0.1504\n",
            "Epoch: 1/2... Training loss: 0.1421\n",
            "Epoch: 1/2... Training loss: 0.1316\n",
            "Epoch: 1/2... Training loss: 0.1245\n",
            "Epoch: 1/2... Training loss: 0.1255\n",
            "Epoch: 1/2... Training loss: 0.1189\n",
            "Epoch: 1/2... Training loss: 0.1198\n",
            "Epoch: 1/2... Training loss: 0.1206\n",
            "Epoch: 1/2... Training loss: 0.1205\n",
            "Epoch: 1/2... Training loss: 0.1131\n",
            "Epoch: 1/2... Training loss: 0.1144\n",
            "Epoch: 1/2... Training loss: 0.1160\n",
            "Epoch: 1/2... Training loss: 0.1146\n",
            "Epoch: 1/2... Training loss: 0.1139\n",
            "Epoch: 1/2... Training loss: 0.1066\n",
            "Epoch: 1/2... Training loss: 0.1150\n",
            "Epoch: 1/2... Training loss: 0.1138\n",
            "Epoch: 1/2... Training loss: 0.1129\n",
            "Epoch: 1/2... Training loss: 0.1133\n",
            "Epoch: 1/2... Training loss: 0.1192\n",
            "Epoch: 1/2... Training loss: 0.1196\n",
            "Epoch: 1/2... Training loss: 0.1138\n",
            "Epoch: 1/2... Training loss: 0.1107\n",
            "Epoch: 1/2... Training loss: 0.1132\n",
            "Epoch: 1/2... Training loss: 0.1098\n",
            "Epoch: 1/2... Training loss: 0.1107\n",
            "Epoch: 1/2... Training loss: 0.1165\n",
            "Epoch: 1/2... Training loss: 0.1078\n",
            "Epoch: 1/2... Training loss: 0.1075\n",
            "Epoch: 1/2... Training loss: 0.1102\n",
            "Epoch: 1/2... Training loss: 0.1152\n",
            "Epoch: 1/2... Training loss: 0.1134\n",
            "Epoch: 1/2... Training loss: 0.1145\n",
            "Epoch: 1/2... Training loss: 0.1109\n",
            "Epoch: 1/2... Training loss: 0.1096\n",
            "Epoch: 1/2... Training loss: 0.1111\n",
            "Epoch: 1/2... Training loss: 0.1092\n",
            "Epoch: 1/2... Training loss: 0.1131\n",
            "Epoch: 1/2... Training loss: 0.1167\n",
            "Epoch: 1/2... Training loss: 0.1182\n",
            "Epoch: 1/2... Training loss: 0.1106\n",
            "Epoch: 1/2... Training loss: 0.1132\n",
            "Epoch: 1/2... Training loss: 0.1116\n",
            "Epoch: 1/2... Training loss: 0.1121\n",
            "Epoch: 1/2... Training loss: 0.1112\n",
            "Epoch: 1/2... Training loss: 0.1114\n",
            "Epoch: 1/2... Training loss: 0.1100\n",
            "Epoch: 1/2... Training loss: 0.1104\n",
            "Epoch: 1/2... Training loss: 0.1119\n",
            "Epoch: 1/2... Training loss: 0.1129\n",
            "Epoch: 1/2... Training loss: 0.1101\n",
            "Epoch: 1/2... Training loss: 0.1151\n",
            "Epoch: 1/2... Training loss: 0.1093\n",
            "Epoch: 1/2... Training loss: 0.1143\n",
            "Epoch: 1/2... Training loss: 0.1103\n",
            "Epoch: 1/2... Training loss: 0.1135\n",
            "Epoch: 1/2... Training loss: 0.1155\n",
            "Epoch: 1/2... Training loss: 0.1126\n",
            "Epoch: 1/2... Training loss: 0.1112\n",
            "Epoch: 1/2... Training loss: 0.1171\n",
            "Epoch: 1/2... Training loss: 0.1106\n",
            "Epoch: 1/2... Training loss: 0.1101\n",
            "Epoch: 1/2... Training loss: 0.1128\n",
            "Epoch: 1/2... Training loss: 0.1194\n",
            "Epoch: 1/2... Training loss: 0.1112\n",
            "Epoch: 1/2... Training loss: 0.1131\n",
            "Epoch: 1/2... Training loss: 0.1115\n",
            "Epoch: 1/2... Training loss: 0.1110\n",
            "Epoch: 1/2... Training loss: 0.1133\n",
            "Epoch: 1/2... Training loss: 0.1125\n",
            "Epoch: 1/2... Training loss: 0.1121\n",
            "Epoch: 1/2... Training loss: 0.1156\n",
            "Epoch: 1/2... Training loss: 0.1154\n",
            "Epoch: 1/2... Training loss: 0.1088\n",
            "Epoch: 1/2... Training loss: 0.1092\n",
            "Epoch: 1/2... Training loss: 0.1095\n",
            "Epoch: 1/2... Training loss: 0.1116\n",
            "Epoch: 1/2... Training loss: 0.1109\n",
            "Epoch: 1/2... Training loss: 0.1114\n",
            "Epoch: 1/2... Training loss: 0.1116\n",
            "Epoch: 1/2... Training loss: 0.1109\n",
            "Epoch: 1/2... Training loss: 0.1078\n",
            "Epoch: 1/2... Training loss: 0.1129\n",
            "Epoch: 1/2... Training loss: 0.1137\n",
            "Epoch: 1/2... Training loss: 0.1111\n",
            "Epoch: 1/2... Training loss: 0.1098\n",
            "Epoch: 1/2... Training loss: 0.1141\n",
            "Epoch: 1/2... Training loss: 0.1074\n",
            "Epoch: 1/2... Training loss: 0.1103\n",
            "Epoch: 1/2... Training loss: 0.1118\n",
            "Epoch: 1/2... Training loss: 0.1121\n",
            "Epoch: 1/2... Training loss: 0.1108\n",
            "Epoch: 1/2... Training loss: 0.1083\n",
            "Epoch: 1/2... Training loss: 0.1167\n",
            "Epoch: 1/2... Training loss: 0.1084\n",
            "Epoch: 1/2... Training loss: 0.1139\n",
            "Epoch: 1/2... Training loss: 0.1089\n",
            "Epoch: 1/2... Training loss: 0.1120\n",
            "Epoch: 1/2... Training loss: 0.1138\n",
            "Epoch: 1/2... Training loss: 0.1115\n",
            "Epoch: 1/2... Training loss: 0.1119\n",
            "Epoch: 1/2... Training loss: 0.1192\n",
            "Epoch: 1/2... Training loss: 0.1099\n",
            "Epoch: 1/2... Training loss: 0.1048\n",
            "Epoch: 1/2... Training loss: 0.1105\n",
            "Epoch: 1/2... Training loss: 0.1109\n",
            "Epoch: 1/2... Training loss: 0.1078\n",
            "Epoch: 1/2... Training loss: 0.1150\n",
            "Epoch: 1/2... Training loss: 0.1072\n",
            "Epoch: 1/2... Training loss: 0.1051\n",
            "Epoch: 1/2... Training loss: 0.1133\n",
            "Epoch: 1/2... Training loss: 0.1065\n",
            "Epoch: 1/2... Training loss: 0.1123\n",
            "Epoch: 1/2... Training loss: 0.1144\n",
            "Epoch: 1/2... Training loss: 0.1121\n",
            "Epoch: 1/2... Training loss: 0.1183\n",
            "Epoch: 1/2... Training loss: 0.1095\n",
            "Epoch: 1/2... Training loss: 0.1112\n",
            "Epoch: 1/2... Training loss: 0.1120\n",
            "Epoch: 1/2... Training loss: 0.1122\n",
            "Epoch: 1/2... Training loss: 0.1037\n",
            "Epoch: 1/2... Training loss: 0.1126\n",
            "Epoch: 1/2... Training loss: 0.1111\n",
            "Epoch: 1/2... Training loss: 0.1117\n",
            "Epoch: 1/2... Training loss: 0.1095\n",
            "Epoch: 1/2... Training loss: 0.1130\n",
            "Epoch: 1/2... Training loss: 0.1108\n",
            "Epoch: 1/2... Training loss: 0.1053\n",
            "Epoch: 1/2... Training loss: 0.1097\n",
            "Epoch: 1/2... Training loss: 0.1162\n",
            "Epoch: 1/2... Training loss: 0.1125\n",
            "Epoch: 1/2... Training loss: 0.1079\n",
            "Epoch: 1/2... Training loss: 0.1132\n",
            "Epoch: 1/2... Training loss: 0.1131\n",
            "Epoch: 1/2... Training loss: 0.1110\n",
            "Epoch: 1/2... Training loss: 0.1149\n",
            "Epoch: 1/2... Training loss: 0.1136\n",
            "Epoch: 1/2... Training loss: 0.1172\n",
            "Epoch: 1/2... Training loss: 0.1126\n",
            "Epoch: 1/2... Training loss: 0.1130\n",
            "Epoch: 1/2... Training loss: 0.1056\n",
            "Epoch: 1/2... Training loss: 0.1117\n",
            "Epoch: 1/2... Training loss: 0.1146\n",
            "Epoch: 1/2... Training loss: 0.1110\n",
            "Epoch: 1/2... Training loss: 0.1118\n",
            "Epoch: 1/2... Training loss: 0.1101\n",
            "Epoch: 1/2... Training loss: 0.1127\n",
            "Epoch: 1/2... Training loss: 0.1187\n",
            "Epoch: 1/2... Training loss: 0.1133\n",
            "Epoch: 1/2... Training loss: 0.1108\n",
            "Epoch: 1/2... Training loss: 0.1131\n",
            "Epoch: 1/2... Training loss: 0.1149\n",
            "Epoch: 1/2... Training loss: 0.1115\n",
            "Epoch: 1/2... Training loss: 0.1148\n",
            "Epoch: 1/2... Training loss: 0.1130\n",
            "Epoch: 1/2... Training loss: 0.1083\n",
            "Epoch: 1/2... Training loss: 0.1101\n",
            "Epoch: 1/2... Training loss: 0.1124\n",
            "Epoch: 1/2... Training loss: 0.1087\n",
            "Epoch: 1/2... Training loss: 0.1111\n",
            "Epoch: 1/2... Training loss: 0.1099\n",
            "Epoch: 1/2... Training loss: 0.1129\n",
            "Epoch: 1/2... Training loss: 0.1135\n",
            "Epoch: 1/2... Training loss: 0.1141\n",
            "Epoch: 1/2... Training loss: 0.1091\n",
            "Epoch: 1/2... Training loss: 0.1129\n",
            "Epoch: 1/2... Training loss: 0.1175\n",
            "Epoch: 1/2... Training loss: 0.1110\n",
            "Epoch: 1/2... Training loss: 0.1058\n",
            "Epoch: 1/2... Training loss: 0.1107\n",
            "Epoch: 1/2... Training loss: 0.1118\n",
            "Epoch: 1/2... Training loss: 0.1113\n",
            "Epoch: 1/2... Training loss: 0.1094\n",
            "Epoch: 1/2... Training loss: 0.1067\n",
            "Epoch: 1/2... Training loss: 0.1162\n",
            "Epoch: 1/2... Training loss: 0.1071\n",
            "Epoch: 1/2... Training loss: 0.1115\n",
            "Epoch: 1/2... Training loss: 0.1147\n",
            "Epoch: 1/2... Training loss: 0.1107\n",
            "Epoch: 1/2... Training loss: 0.1098\n",
            "Epoch: 1/2... Training loss: 0.1112\n",
            "Epoch: 1/2... Training loss: 0.1131\n",
            "Epoch: 1/2... Training loss: 0.1133\n",
            "Epoch: 1/2... Training loss: 0.1113\n",
            "Epoch: 1/2... Training loss: 0.1129\n",
            "Epoch: 1/2... Training loss: 0.1062\n",
            "Epoch: 1/2... Training loss: 0.1115\n",
            "Epoch: 1/2... Training loss: 0.1126\n",
            "Epoch: 1/2... Training loss: 0.1147\n",
            "Epoch: 1/2... Training loss: 0.1161\n",
            "Epoch: 1/2... Training loss: 0.1164\n",
            "Epoch: 1/2... Training loss: 0.1114\n",
            "Epoch: 1/2... Training loss: 0.1144\n",
            "Epoch: 1/2... Training loss: 0.1120\n",
            "Epoch: 1/2... Training loss: 0.1098\n",
            "Epoch: 1/2... Training loss: 0.1092\n",
            "Epoch: 1/2... Training loss: 0.1062\n",
            "Epoch: 1/2... Training loss: 0.1071\n",
            "Epoch: 1/2... Training loss: 0.1088\n",
            "Epoch: 1/2... Training loss: 0.1089\n",
            "Epoch: 1/2... Training loss: 0.1105\n",
            "Epoch: 1/2... Training loss: 0.1152\n",
            "Epoch: 1/2... Training loss: 0.1111\n",
            "Epoch: 1/2... Training loss: 0.1113\n",
            "Epoch: 1/2... Training loss: 0.1133\n",
            "Epoch: 1/2... Training loss: 0.1103\n",
            "Epoch: 1/2... Training loss: 0.1149\n",
            "Epoch: 1/2... Training loss: 0.1127\n",
            "Epoch: 1/2... Training loss: 0.1125\n",
            "Epoch: 1/2... Training loss: 0.1132\n",
            "Epoch: 1/2... Training loss: 0.1158\n",
            "Epoch: 1/2... Training loss: 0.1186\n",
            "Epoch: 1/2... Training loss: 0.1113\n",
            "Epoch: 1/2... Training loss: 0.1110\n",
            "Epoch: 1/2... Training loss: 0.1172\n",
            "Epoch: 1/2... Training loss: 0.1071\n",
            "Epoch: 1/2... Training loss: 0.1095\n",
            "Epoch: 1/2... Training loss: 0.1104\n",
            "Epoch: 1/2... Training loss: 0.1159\n",
            "Epoch: 1/2... Training loss: 0.1116\n",
            "Epoch: 1/2... Training loss: 0.1129\n",
            "Epoch: 1/2... Training loss: 0.1114\n",
            "Epoch: 1/2... Training loss: 0.1123\n",
            "Epoch: 1/2... Training loss: 0.1167\n",
            "Epoch: 1/2... Training loss: 0.1092\n",
            "Epoch: 1/2... Training loss: 0.1128\n",
            "Epoch: 1/2... Training loss: 0.1117\n",
            "Epoch: 1/2... Training loss: 0.1151\n",
            "Epoch: 1/2... Training loss: 0.1175\n",
            "Epoch: 1/2... Training loss: 0.1093\n",
            "Epoch: 1/2... Training loss: 0.1111\n",
            "Epoch: 1/2... Training loss: 0.1157\n",
            "Epoch: 1/2... Training loss: 0.1115\n",
            "Epoch: 1/2... Training loss: 0.1130\n",
            "Epoch: 1/2... Training loss: 0.1169\n",
            "Epoch: 1/2... Training loss: 0.1084\n",
            "Epoch: 1/2... Training loss: 0.1058\n",
            "Epoch: 1/2... Training loss: 0.1120\n",
            "Epoch: 1/2... Training loss: 0.1139\n",
            "Epoch: 1/2... Training loss: 0.1141\n",
            "Epoch: 1/2... Training loss: 0.1101\n",
            "Epoch: 1/2... Training loss: 0.1098\n",
            "Epoch: 1/2... Training loss: 0.1125\n",
            "Epoch: 1/2... Training loss: 0.1158\n",
            "Epoch: 1/2... Training loss: 0.1129\n",
            "Epoch: 1/2... Training loss: 0.1124\n",
            "Epoch: 1/2... Training loss: 0.1180\n",
            "Epoch: 1/2... Training loss: 0.1117\n",
            "Epoch: 1/2... Training loss: 0.1127\n",
            "Epoch: 1/2... Training loss: 0.1134\n",
            "Epoch: 1/2... Training loss: 0.1104\n",
            "Epoch: 1/2... Training loss: 0.1138\n",
            "Epoch: 1/2... Training loss: 0.1176\n",
            "Epoch: 1/2... Training loss: 0.1129\n",
            "Epoch: 1/2... Training loss: 0.1128\n",
            "Epoch: 1/2... Training loss: 0.1104\n",
            "Epoch: 1/2... Training loss: 0.1191\n",
            "Epoch: 1/2... Training loss: 0.1151\n",
            "Epoch: 1/2... Training loss: 0.1118\n",
            "Epoch: 1/2... Training loss: 0.1128\n",
            "Epoch: 1/2... Training loss: 0.1117\n",
            "Epoch: 2/2... Training loss: 0.1134\n",
            "Epoch: 2/2... Training loss: 0.1104\n",
            "Epoch: 2/2... Training loss: 0.1086\n",
            "Epoch: 2/2... Training loss: 0.1133\n",
            "Epoch: 2/2... Training loss: 0.1152\n",
            "Epoch: 2/2... Training loss: 0.1127\n",
            "Epoch: 2/2... Training loss: 0.1167\n",
            "Epoch: 2/2... Training loss: 0.1099\n",
            "Epoch: 2/2... Training loss: 0.1185\n",
            "Epoch: 2/2... Training loss: 0.1093\n",
            "Epoch: 2/2... Training loss: 0.1071\n",
            "Epoch: 2/2... Training loss: 0.1137\n",
            "Epoch: 2/2... Training loss: 0.1136\n",
            "Epoch: 2/2... Training loss: 0.1115\n",
            "Epoch: 2/2... Training loss: 0.1093\n",
            "Epoch: 2/2... Training loss: 0.1111\n",
            "Epoch: 2/2... Training loss: 0.1115\n",
            "Epoch: 2/2... Training loss: 0.1100\n",
            "Epoch: 2/2... Training loss: 0.1106\n",
            "Epoch: 2/2... Training loss: 0.1136\n",
            "Epoch: 2/2... Training loss: 0.1110\n",
            "Epoch: 2/2... Training loss: 0.1170\n",
            "Epoch: 2/2... Training loss: 0.1107\n",
            "Epoch: 2/2... Training loss: 0.1126\n",
            "Epoch: 2/2... Training loss: 0.1138\n",
            "Epoch: 2/2... Training loss: 0.1094\n",
            "Epoch: 2/2... Training loss: 0.1120\n",
            "Epoch: 2/2... Training loss: 0.1133\n",
            "Epoch: 2/2... Training loss: 0.1100\n",
            "Epoch: 2/2... Training loss: 0.1097\n",
            "Epoch: 2/2... Training loss: 0.1126\n",
            "Epoch: 2/2... Training loss: 0.1119\n",
            "Epoch: 2/2... Training loss: 0.1097\n",
            "Epoch: 2/2... Training loss: 0.1176\n",
            "Epoch: 2/2... Training loss: 0.1120\n",
            "Epoch: 2/2... Training loss: 0.1134\n",
            "Epoch: 2/2... Training loss: 0.1157\n",
            "Epoch: 2/2... Training loss: 0.1074\n",
            "Epoch: 2/2... Training loss: 0.1072\n",
            "Epoch: 2/2... Training loss: 0.1098\n",
            "Epoch: 2/2... Training loss: 0.1141\n",
            "Epoch: 2/2... Training loss: 0.1119\n",
            "Epoch: 2/2... Training loss: 0.1125\n",
            "Epoch: 2/2... Training loss: 0.1095\n",
            "Epoch: 2/2... Training loss: 0.1130\n",
            "Epoch: 2/2... Training loss: 0.1087\n",
            "Epoch: 2/2... Training loss: 0.1083\n",
            "Epoch: 2/2... Training loss: 0.1197\n",
            "Epoch: 2/2... Training loss: 0.1126\n",
            "Epoch: 2/2... Training loss: 0.1083\n",
            "Epoch: 2/2... Training loss: 0.1128\n",
            "Epoch: 2/2... Training loss: 0.1173\n",
            "Epoch: 2/2... Training loss: 0.1101\n",
            "Epoch: 2/2... Training loss: 0.1090\n",
            "Epoch: 2/2... Training loss: 0.1156\n",
            "Epoch: 2/2... Training loss: 0.1115\n",
            "Epoch: 2/2... Training loss: 0.1116\n",
            "Epoch: 2/2... Training loss: 0.1144\n",
            "Epoch: 2/2... Training loss: 0.1117\n",
            "Epoch: 2/2... Training loss: 0.1103\n",
            "Epoch: 2/2... Training loss: 0.1191\n",
            "Epoch: 2/2... Training loss: 0.1151\n",
            "Epoch: 2/2... Training loss: 0.1127\n",
            "Epoch: 2/2... Training loss: 0.1121\n",
            "Epoch: 2/2... Training loss: 0.1128\n",
            "Epoch: 2/2... Training loss: 0.1115\n",
            "Epoch: 2/2... Training loss: 0.1097\n",
            "Epoch: 2/2... Training loss: 0.1092\n",
            "Epoch: 2/2... Training loss: 0.1120\n",
            "Epoch: 2/2... Training loss: 0.1155\n",
            "Epoch: 2/2... Training loss: 0.1161\n",
            "Epoch: 2/2... Training loss: 0.1102\n",
            "Epoch: 2/2... Training loss: 0.1151\n",
            "Epoch: 2/2... Training loss: 0.1107\n",
            "Epoch: 2/2... Training loss: 0.1087\n",
            "Epoch: 2/2... Training loss: 0.1088\n",
            "Epoch: 2/2... Training loss: 0.1165\n",
            "Epoch: 2/2... Training loss: 0.1101\n",
            "Epoch: 2/2... Training loss: 0.1104\n",
            "Epoch: 2/2... Training loss: 0.1101\n",
            "Epoch: 2/2... Training loss: 0.1103\n",
            "Epoch: 2/2... Training loss: 0.1101\n",
            "Epoch: 2/2... Training loss: 0.1148\n",
            "Epoch: 2/2... Training loss: 0.1175\n",
            "Epoch: 2/2... Training loss: 0.1143\n",
            "Epoch: 2/2... Training loss: 0.1140\n",
            "Epoch: 2/2... Training loss: 0.1123\n",
            "Epoch: 2/2... Training loss: 0.1139\n",
            "Epoch: 2/2... Training loss: 0.1127\n",
            "Epoch: 2/2... Training loss: 0.1138\n",
            "Epoch: 2/2... Training loss: 0.1135\n",
            "Epoch: 2/2... Training loss: 0.1088\n",
            "Epoch: 2/2... Training loss: 0.1171\n",
            "Epoch: 2/2... Training loss: 0.1155\n",
            "Epoch: 2/2... Training loss: 0.1136\n",
            "Epoch: 2/2... Training loss: 0.1121\n",
            "Epoch: 2/2... Training loss: 0.1155\n",
            "Epoch: 2/2... Training loss: 0.1112\n",
            "Epoch: 2/2... Training loss: 0.1104\n",
            "Epoch: 2/2... Training loss: 0.1116\n",
            "Epoch: 2/2... Training loss: 0.1138\n",
            "Epoch: 2/2... Training loss: 0.1096\n",
            "Epoch: 2/2... Training loss: 0.1021\n",
            "Epoch: 2/2... Training loss: 0.1178\n",
            "Epoch: 2/2... Training loss: 0.1136\n",
            "Epoch: 2/2... Training loss: 0.1137\n",
            "Epoch: 2/2... Training loss: 0.1113\n",
            "Epoch: 2/2... Training loss: 0.1128\n",
            "Epoch: 2/2... Training loss: 0.1139\n",
            "Epoch: 2/2... Training loss: 0.1163\n",
            "Epoch: 2/2... Training loss: 0.1085\n",
            "Epoch: 2/2... Training loss: 0.1186\n",
            "Epoch: 2/2... Training loss: 0.1140\n",
            "Epoch: 2/2... Training loss: 0.1127\n",
            "Epoch: 2/2... Training loss: 0.1118\n",
            "Epoch: 2/2... Training loss: 0.1147\n",
            "Epoch: 2/2... Training loss: 0.1141\n",
            "Epoch: 2/2... Training loss: 0.1149\n",
            "Epoch: 2/2... Training loss: 0.1154\n",
            "Epoch: 2/2... Training loss: 0.1199\n",
            "Epoch: 2/2... Training loss: 0.1120\n",
            "Epoch: 2/2... Training loss: 0.1081\n",
            "Epoch: 2/2... Training loss: 0.1135\n",
            "Epoch: 2/2... Training loss: 0.1089\n",
            "Epoch: 2/2... Training loss: 0.1129\n",
            "Epoch: 2/2... Training loss: 0.1114\n",
            "Epoch: 2/2... Training loss: 0.1105\n",
            "Epoch: 2/2... Training loss: 0.1102\n",
            "Epoch: 2/2... Training loss: 0.1155\n",
            "Epoch: 2/2... Training loss: 0.1119\n",
            "Epoch: 2/2... Training loss: 0.1160\n",
            "Epoch: 2/2... Training loss: 0.1112\n",
            "Epoch: 2/2... Training loss: 0.1140\n",
            "Epoch: 2/2... Training loss: 0.1110\n",
            "Epoch: 2/2... Training loss: 0.1158\n",
            "Epoch: 2/2... Training loss: 0.1136\n",
            "Epoch: 2/2... Training loss: 0.1088\n",
            "Epoch: 2/2... Training loss: 0.1106\n",
            "Epoch: 2/2... Training loss: 0.1148\n",
            "Epoch: 2/2... Training loss: 0.1103\n",
            "Epoch: 2/2... Training loss: 0.1109\n",
            "Epoch: 2/2... Training loss: 0.1111\n",
            "Epoch: 2/2... Training loss: 0.1102\n",
            "Epoch: 2/2... Training loss: 0.1090\n",
            "Epoch: 2/2... Training loss: 0.1146\n",
            "Epoch: 2/2... Training loss: 0.1111\n",
            "Epoch: 2/2... Training loss: 0.1183\n",
            "Epoch: 2/2... Training loss: 0.1099\n",
            "Epoch: 2/2... Training loss: 0.1150\n",
            "Epoch: 2/2... Training loss: 0.1063\n",
            "Epoch: 2/2... Training loss: 0.1111\n",
            "Epoch: 2/2... Training loss: 0.1122\n",
            "Epoch: 2/2... Training loss: 0.1200\n",
            "Epoch: 2/2... Training loss: 0.1118\n",
            "Epoch: 2/2... Training loss: 0.1158\n",
            "Epoch: 2/2... Training loss: 0.1117\n",
            "Epoch: 2/2... Training loss: 0.1134\n",
            "Epoch: 2/2... Training loss: 0.1159\n",
            "Epoch: 2/2... Training loss: 0.1109\n",
            "Epoch: 2/2... Training loss: 0.1093\n",
            "Epoch: 2/2... Training loss: 0.1136\n",
            "Epoch: 2/2... Training loss: 0.1105\n",
            "Epoch: 2/2... Training loss: 0.1128\n",
            "Epoch: 2/2... Training loss: 0.1105\n",
            "Epoch: 2/2... Training loss: 0.1109\n",
            "Epoch: 2/2... Training loss: 0.1133\n",
            "Epoch: 2/2... Training loss: 0.1104\n",
            "Epoch: 2/2... Training loss: 0.1077\n",
            "Epoch: 2/2... Training loss: 0.1057\n",
            "Epoch: 2/2... Training loss: 0.1076\n",
            "Epoch: 2/2... Training loss: 0.1106\n",
            "Epoch: 2/2... Training loss: 0.1145\n",
            "Epoch: 2/2... Training loss: 0.1140\n",
            "Epoch: 2/2... Training loss: 0.1096\n",
            "Epoch: 2/2... Training loss: 0.1114\n",
            "Epoch: 2/2... Training loss: 0.1138\n",
            "Epoch: 2/2... Training loss: 0.1081\n",
            "Epoch: 2/2... Training loss: 0.1104\n",
            "Epoch: 2/2... Training loss: 0.1127\n",
            "Epoch: 2/2... Training loss: 0.1132\n",
            "Epoch: 2/2... Training loss: 0.1103\n",
            "Epoch: 2/2... Training loss: 0.1106\n",
            "Epoch: 2/2... Training loss: 0.1105\n",
            "Epoch: 2/2... Training loss: 0.1101\n",
            "Epoch: 2/2... Training loss: 0.1149\n",
            "Epoch: 2/2... Training loss: 0.1115\n",
            "Epoch: 2/2... Training loss: 0.1097\n",
            "Epoch: 2/2... Training loss: 0.1095\n",
            "Epoch: 2/2... Training loss: 0.1114\n",
            "Epoch: 2/2... Training loss: 0.1143\n",
            "Epoch: 2/2... Training loss: 0.1114\n",
            "Epoch: 2/2... Training loss: 0.1127\n",
            "Epoch: 2/2... Training loss: 0.1110\n",
            "Epoch: 2/2... Training loss: 0.1087\n",
            "Epoch: 2/2... Training loss: 0.1099\n",
            "Epoch: 2/2... Training loss: 0.1163\n",
            "Epoch: 2/2... Training loss: 0.1105\n",
            "Epoch: 2/2... Training loss: 0.1103\n",
            "Epoch: 2/2... Training loss: 0.1103\n",
            "Epoch: 2/2... Training loss: 0.1099\n",
            "Epoch: 2/2... Training loss: 0.1061\n",
            "Epoch: 2/2... Training loss: 0.1096\n",
            "Epoch: 2/2... Training loss: 0.1114\n",
            "Epoch: 2/2... Training loss: 0.1076\n",
            "Epoch: 2/2... Training loss: 0.1152\n",
            "Epoch: 2/2... Training loss: 0.1068\n",
            "Epoch: 2/2... Training loss: 0.1111\n",
            "Epoch: 2/2... Training loss: 0.1122\n",
            "Epoch: 2/2... Training loss: 0.1077\n",
            "Epoch: 2/2... Training loss: 0.1144\n",
            "Epoch: 2/2... Training loss: 0.1058\n",
            "Epoch: 2/2... Training loss: 0.1126\n",
            "Epoch: 2/2... Training loss: 0.1111\n",
            "Epoch: 2/2... Training loss: 0.1118\n",
            "Epoch: 2/2... Training loss: 0.1101\n",
            "Epoch: 2/2... Training loss: 0.1088\n",
            "Epoch: 2/2... Training loss: 0.1112\n",
            "Epoch: 2/2... Training loss: 0.1133\n",
            "Epoch: 2/2... Training loss: 0.1068\n",
            "Epoch: 2/2... Training loss: 0.1123\n",
            "Epoch: 2/2... Training loss: 0.1143\n",
            "Epoch: 2/2... Training loss: 0.1096\n",
            "Epoch: 2/2... Training loss: 0.1127\n",
            "Epoch: 2/2... Training loss: 0.1086\n",
            "Epoch: 2/2... Training loss: 0.1101\n",
            "Epoch: 2/2... Training loss: 0.1117\n",
            "Epoch: 2/2... Training loss: 0.1066\n",
            "Epoch: 2/2... Training loss: 0.1123\n",
            "Epoch: 2/2... Training loss: 0.1112\n",
            "Epoch: 2/2... Training loss: 0.1103\n",
            "Epoch: 2/2... Training loss: 0.1086\n",
            "Epoch: 2/2... Training loss: 0.1174\n",
            "Epoch: 2/2... Training loss: 0.1148\n",
            "Epoch: 2/2... Training loss: 0.1060\n",
            "Epoch: 2/2... Training loss: 0.1095\n",
            "Epoch: 2/2... Training loss: 0.1110\n",
            "Epoch: 2/2... Training loss: 0.1094\n",
            "Epoch: 2/2... Training loss: 0.1155\n",
            "Epoch: 2/2... Training loss: 0.1144\n",
            "Epoch: 2/2... Training loss: 0.1122\n",
            "Epoch: 2/2... Training loss: 0.1153\n",
            "Epoch: 2/2... Training loss: 0.1149\n",
            "Epoch: 2/2... Training loss: 0.1089\n",
            "Epoch: 2/2... Training loss: 0.1127\n",
            "Epoch: 2/2... Training loss: 0.1142\n",
            "Epoch: 2/2... Training loss: 0.1106\n",
            "Epoch: 2/2... Training loss: 0.1135\n",
            "Epoch: 2/2... Training loss: 0.1168\n",
            "Epoch: 2/2... Training loss: 0.1163\n",
            "Epoch: 2/2... Training loss: 0.1115\n",
            "Epoch: 2/2... Training loss: 0.1100\n",
            "Epoch: 2/2... Training loss: 0.1109\n",
            "Epoch: 2/2... Training loss: 0.1160\n",
            "Epoch: 2/2... Training loss: 0.1144\n",
            "Epoch: 2/2... Training loss: 0.1113\n",
            "Epoch: 2/2... Training loss: 0.1148\n",
            "Epoch: 2/2... Training loss: 0.1133\n",
            "Epoch: 2/2... Training loss: 0.1135\n",
            "Epoch: 2/2... Training loss: 0.1167\n",
            "Epoch: 2/2... Training loss: 0.1121\n",
            "Epoch: 2/2... Training loss: 0.1122\n",
            "Epoch: 2/2... Training loss: 0.1129\n",
            "Epoch: 2/2... Training loss: 0.1095\n",
            "Epoch: 2/2... Training loss: 0.1143\n",
            "Epoch: 2/2... Training loss: 0.1161\n",
            "Epoch: 2/2... Training loss: 0.1093\n",
            "Epoch: 2/2... Training loss: 0.1136\n",
            "Epoch: 2/2... Training loss: 0.1171\n",
            "Epoch: 2/2... Training loss: 0.1098\n",
            "Epoch: 2/2... Training loss: 0.1095\n",
            "Epoch: 2/2... Training loss: 0.1128\n",
            "Epoch: 2/2... Training loss: 0.1097\n",
            "Epoch: 2/2... Training loss: 0.1090\n",
            "Epoch: 2/2... Training loss: 0.1109\n",
            "Epoch: 2/2... Training loss: 0.1130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TD6oWkXYqa8",
        "colab_type": "text"
      },
      "source": [
        "<h3>Plotting the digits</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9YRUP5UYqa8",
        "colab_type": "code",
        "colab": {},
        "outputId": "16e6fbb1-742d-4450-d8e0-8ec946074853"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
        "in_imgs = mnist.test.images[:10]\n",
        "reconstructed = sess.run(convolution7, feed_dict={inputs_dim: in_imgs.reshape((10, 28, 28, 1))})\n",
        "\n",
        "for images, row in zip([in_imgs, reconstructed], axes):\n",
        "    for img, ax in zip(images, row):\n",
        "        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "fig.tight_layout(pad=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaW2im1WYqa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}