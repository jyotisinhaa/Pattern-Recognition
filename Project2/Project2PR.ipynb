{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2PR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd9ySmRtB1Ii",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "<h2><strong>Linear Discriminant Functions and Support Vector Machines</strong></h2>\n",
        "\n",
        "<h4><strong>JYOTI SINHA </strong></h4>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWXBLBReCjO9",
        "colab_type": "text"
      },
      "source": [
        "<h4><strong>Problem 1</strong></h4>\n",
        "\n",
        "<strong>We train a Support Vector classifier using the MNIST training dataset and report performance using MNIST testing dataset¶\n",
        "</strong>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjx8l8NOJYqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing Library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBtCCFw5KHjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the data set \n",
        "df=tf.keras.datasets.mnist\n",
        "(x_train,y_train),(x_test,y_test)=df.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzz3HF7DVRv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalizing the dataset\n",
        "x_train=x_train/255\n",
        "x_test=x_test/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFYDu6n7KWnE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8a2fb9f-9603-400e-fc6b-6a5fad94a36e"
      },
      "source": [
        "#changing the shape of the dataset\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test  = x_test.reshape(10000, 784)\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4rywpCcKsgW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0239bc9b-134e-49ee-b663-d2f2e9cf02c5"
      },
      "source": [
        "#SVM Model\n",
        "model=SVC(gamma='scale',C=1.0,kernel='linear')\n",
        "model.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IZF9sPDT9PA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c8d3e22-a61c-4946-e049-0b54abb58e8a"
      },
      "source": [
        "\n",
        "#Finding the accuracy:-\n",
        "from sklearn import svm, metrics\n",
        "y_prediction= model.predict(x_test)\n",
        "print('Accuracy: ',metrics.accuracy_score(y_prediction,y_test)*100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  94.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSw41wk4Ueqd",
        "colab_type": "text"
      },
      "source": [
        "<h4><strong>Problem 2:-</strong></h4>\n",
        "\n",
        "Our Goal is to minimize \n",
        ">>$\n",
        " \\frac{1}{2}w^T.w+C \\sum_{n=1}^{N} \\xi_{i}\n",
        "\\\\\n",
        "\\text{Subject to}: y_i.(w^T.x_i) \\geq {1-\\xi_{i}} \\text{ and } \\xi_i \\geq 0 \\text{ for i }=1,...,N\n",
        "$\n",
        "\n",
        "\n",
        "\n",
        "The margin is given by :\n",
        " $\\gamma = \\frac{1}{\\sqrt{w^T.w +C \\sum_{i=1}^{N} \\xi_i }}$\n",
        "\n",
        "We know that the Langrange function is given by,\n",
        "\\begin{align}\n",
        "{\\mathcal{L}} = \\frac{1}{2} {\\overrightarrow{w}}^T\\overrightarrow{w} + C \\sum_{i=1}^N\\xi_i + \\Sigma_i\\alpha_i(1-y_i.w^Tx_i -\\xi_i) - \\sum_{i=1}^N \\beta_i \\xi_i\n",
        "\\end{align}\n",
        "<br><br>\n",
        "Where Lagrange multipliers should be  $\\alpha \\geq 0$ and $ \\beta \\geq 0$,<br>\n",
        "Inequality constraints $1 - y_i(w^Tx_i) - \\xi_i \\leq 0$ and $\\xi_i \\geq 0$ for $i=1,...,N$\n",
        "<br><br>\n",
        "####<h2> Claim: <br></h2>\n",
        "$\\underbrace{\\underset{\\alpha,\\beta}{max} \\underset{w,\\xi}{min} \\mathcal{L}}_\\text{dual solution} \\leq \\underbrace{\\underset{w,\\xi}{min} \\underset{\\alpha,\\beta}{max} \\mathcal{L}}_\\text{primal solution}$\n",
        "\n",
        "The Lagrange function, L is given by,\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "{\\mathcal{L}} = \\frac{1}{2} {\\overrightarrow{w}}^T\\overrightarrow{w} + C \\sum_{i=1}^N\\xi_i + \\Sigma_i\\alpha_i(1-y_i.w^Tx_i -\\xi_i) - \\sum_{i=1}^N \\beta_i \\xi_i\n",
        "\\end{align}\n",
        "\n",
        "On expanding the Lagranges function, We get\n",
        "\\begin{align}\n",
        "{\\mathcal{L}} = \\frac{1}{2} {\\overrightarrow{w}}^T\\overrightarrow{w} + C \\sum_{i=1}^N\\xi_i + \\Sigma_i\\alpha_i- \\Sigma_i\\alpha_i(y_i.w^Tx_i) -\\Sigma_i\\alpha_i\\xi_i) - \\sum_{i=1}^N \\beta_i \\xi_i\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\text {On Differentiating the Lagranges function wrt to }\n",
        "\\omega,\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "\\\\\n",
        "  \\frac{\\partial_{\\mathcal{L}}}{\\partial_w} = w - \\sum_i \\alpha_i y_i x_i = 0 \\implies w = \\sum_i \\alpha_i y_i \\overrightarrow{x_i}\n",
        "  \\\\\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\text {On Differentiating the Lagranges function wrt to }\n",
        "\\xi ,\n",
        " \\\\\n",
        "  \\frac{\\partial_{\\mathcal{L}}}{\\partial_\\xi} = 0 \\implies C - \\alpha_i - \\beta_i = 0\n",
        "  \\implies C=\\alpha_i + \\beta_i\n",
        "\\\\implies\\: 0\\leq \\alpha_i \\leq C \\text{ and } \\beta_i \\geq 0\n",
        " \\end{align}\n",
        " \n",
        " \\begin{align}\n",
        "\\text {Substituting the values of  }\n",
        "\\omega \n",
        "\\text{ and  C in Lagranges equation,} \n",
        "\\end{align}\n",
        " \\begin{align}\n",
        "{\\mathcal{L}} = \\frac{1}{2} w^T \\sum_i \\alpha_i y_i \\overrightarrow{x_i} + (\\alpha_i + \\beta_i) \\sum_{i=1}^N\\xi_i + \\Sigma_i\\alpha_i(1-y_i.w^Tx_i -\\xi_i) - \\sum_{i=1}^N \\beta_i \\xi_i\n",
        "\\end{align}\n",
        " \n",
        " \\begin{align}\n",
        "\\text {On expanding the equation further we get,  }\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "\\\\= \\frac{1}{2} w^T \\sum_i \\alpha_i y_i \\overrightarrow{x_i} + \\alpha_i\\sum_{i=1}^N\\xi_i + \\beta_i \\sum_{i=1}^N\\xi_i +\\sum_i \\alpha_i - w^T \\sum_i \\alpha_i y_i \\overrightarrow{x_i} - \\sum_i \\alpha_i \\xi_i - \\sum_{i=1}^N \\beta_i \\xi_i\n",
        "\\end{align}\n",
        " \\begin{align}\n",
        "\\text {On simplifying  the equation further we get,}\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "\\\\=\\sum_i \\alpha_i- \\frac{1}{2} \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j (\\overrightarrow{x_i}^T x_j )\n",
        "\\end{align}\n",
        "Thus, Primal margin is given by :\n",
        " $\\gamma = \\frac{1}{\\sqrt{w^T.w +C \\sum_{i=1}^{N} \\xi_i }}$\n",
        " \n",
        "And  the Dual margin is given by :\n",
        "$\\gamma = \\frac{1}{\\sqrt{\\alpha_i \\alpha_j y_i y_j (x_i^T x_j )}}$\n",
        "\n",
        "<h2>Benefit of Maximizing the margin:-</h2>\n",
        "\n",
        "It is very important to maximize the margin\n",
        "because  the points which are lying near the decision \n",
        "surface represent very uncertain classification decision.\n",
        "We can also say that there is atmost 50% chance of the\n",
        "classifier to decide in either way. Hence, the classifier\n",
        "with a larger margin makes no low uncertainty classification decisions. Thus, A larger margin would help to generalize the model and makes better prediction on any dataset.\n",
        "\n",
        "<h2>Benefit of Dual problem instead Primal problem:-</h2>\n",
        "1. It is easier to optimize in Dual than the Primal when the number of data points is lower than the number of dimensions.\n",
        "\n",
        "2.Dual problems provide lots of computational advantage in the problems which deals with lesser number of variable and a many constraints.\n",
        "\n",
        "3.Dual are more helpful than the Primal for sensitivity analysis.\n",
        "\n",
        "4.It is easier to find the intial feasible solution of the dual than the the primal.\n",
        "\n",
        "<h2>Characterize the support vectors:-</h2>\n",
        "\n",
        "\n",
        "* Points on the margin(ξn = 0) where  margin boundaries are given by  w^T x + b = −1 and w^T x + b = +1\n",
        "\n",
        "* Inside the margin region (0 < ξn < 1) but on the correct side\n",
        "\n",
        "* Lying on the wrong side of the hyperplane (ξn ≥ 1)\n",
        "\n",
        "\n",
        "<h4><strong>Problem 3</strong></h4>\n",
        " The primal problem for multiple class is given by ,\n",
        " \\begin{equation}\n",
        "\\underset{{w_{m}}, \\xi }{min}     \\frac{1}{2} \\sum_{m} ||w_{m}||^2+ C \\sum_{i} \\xi_{i}\n",
        "\\end{equation} \n",
        "<br><br>\n",
        "\\begin{align}\n",
        "{\\text{subject to,   }}  w_{y_i}^Tx_i - w_{m}^{T}x_i \\geq (e_i)^m - \\xi_i \n",
        "\\end{align}\n",
        " for all m, i,\n",
        " \n",
        " where C>0 is the regularization parameter.\n",
        "\\begin{align}\n",
        "{w_{m} \\text{ is the weight vector associated with the class m}}\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "{e_{i}^m=1- \\delta_{y_i,m}}\n",
        "\\end{align}\n",
        "and \n",
        "\\begin{align}\n",
        "{\\delta_{y_i,m}=1  { \\text{  if } }                   \\ y_{i}=m}\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "{\\delta_{y_i,m}=0  { \\text{  if } }                      \\ y_{i}!=m}\n",
        "\\end{align}\n",
        "The  Decision function is: \n",
        "\\begin{align}\n",
        "argmax_m w^{T}_{m}(x)\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "{\\text{The dual problem involves a factor alpha having variables}}\\ {\\alpha_i}^m\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "{\\text{The}} \\  {w_{m}}  \\ {\\text{get  defined via alpha as}}  \n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        " \\ w_{m}(\\alpha)=  \\sum_{i} {\\alpha_i}^m{x_i}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{align}\n",
        "{\\text{let}} \\ {C_{i}^m}=0 \\ { \\text{  if } }                  { \\ y_{i}!=m} \n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "{\\text{And}} \\ {C_{i}^m}=C \\ { \\text{  if } }                  { \\ y_{i}=m} \n",
        "\\end{align}\n",
        "The Dual Problem  is \n",
        "\\begin{equation}\n",
        "\\underset{\\alpha}{min} f(\\alpha)= \\frac{1}{2} \\sum_{m} ||w_{m}(\\alpha)||^2 +  \\sum_{i} \\sum_{m} e_{i}^m {\\alpha_i}^m\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{align}\n",
        "{\\text{subject to,   }}  {\\alpha_i}^m <= {C_{i}^m} ,   \\sum_{m}{\\alpha_i}^m=0\n",
        "\\end{align}\n",
        "The gradient of f plays an important role and is given by,\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        " {g_i}^m =  {w_{m}(\\alpha)^Tx_{i} +  {e_{i}^m} }\n",
        "\\end{align}\n",
        "And\n",
        "\\begin{equation}\n",
        "v_i=\\underset{m}{max}(g_{i})^m - \\underset{m: {\\alpha_i}^m < (C_i)^m}{min}(g_{i})^m \n",
        "\\end{equation} \n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "{\\text{Thus Dual optimality holds when:  } v_i=0} \n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNs7Mg81Y2kE",
        "colab_type": "text"
      },
      "source": [
        "<h3>Reference</h3>\n",
        "1)Slide of the Professor\n",
        "\n",
        "2)Video of the Professor\n",
        "\n",
        "3)Video of Prof Varun Chandola\n",
        "\n",
        "4)https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0042947\n",
        "\n",
        "5)http://ntucsu.csie.ntu.edu.tw/~cjlin/papers/sdm_kdd.pdf\n",
        "\n",
        "6)nlp.stanford.edu"
      ]
    }
  ]
}